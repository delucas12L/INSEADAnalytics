# To-Do's
# 1) Separate Training data (80/20) -> DONE
# 2) Do correlation + data visualization analysis
# 3) Explore different models:
#       a) Linear (with log)
#       b) Lasso (with log)
#       c) random forest
#       d) xgboost


---
title: "DSB_Group1_FinalProjectProposal"
author: "Marcelo De Rada Ocampo, Ollie Phillpot, Miguel Lucas, Prathamesh Dole, Harshul Lilani"
output:
  html_document:
    css: ../AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../AnalyticsStyles/default.sty
always_allow_html: yes
---

# Section 0: Download library/ package
Setup code.

```{r echo=FALSE, message=FALSE}
#make_pdf_file = 0 # SET THIS TO 1 IF WE COMPILE PDF FILE, 0 OTHERWISE (FOR HTML)

#source("../AnalyticsLibraries/library.R")
#source("../AnalyticsLibraries/heatmapOutput.R")

library('ggplot2')
library('ggthemes') 
library('scales')
library('dplyr') 
library('mice')
library('randomForest')
library('data.table')
library('gridExtra')
library('corrplot') 
library('GGally')
library('e1071')
library('caret')
library('glmnet')
#install.packages("tidyverse")
```

# Section 1: The Business Context
Generally the largest investment of a person's life is buying a house. It is an emotional affair and people often overpay. There are also many small businesses that build and sell residential housing, but the construction industry is exceedingly slow to adopt new technological practices. When buying or selling property it is considered advantageous to know the area to develop a "feel" for sale prices. This is unscientific and we believe there is much room for optimisation.

We will participate in this Kaggle competion for a personal reason and a business reason:
1. Miguel is currently looking to buy a house and would like to know potential hidden factors he should look for in order to find a for a good price.
2. Ollie is a shareholder in his father's housing development company, which has recently completed a project and is looking for its next investment opportunity. This project will be used to identify possible features that could add significant value to the next project and improve ROI.

The Kaggle competition can be found [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).

<hr>\clearpage

# The Data
(Data source: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=train.csv. We acknowledge the following:
DeCock, Dean. (2011). UCI Machine Learning Repository [http://jse.amstat.org/v19n3/decock.pdf]. Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project at Truman State University.)

The data set has been generated as an alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. The following is an example of the data/ data library:

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges participants to predict the final price of each home.


# Read the training and testing data
``` {r echo=FALSE, message=FALSE}

#train <-read.csv("train.csv", stringsAsFactors = F)
#test  <-read.csv("test.csv", stringsAsFactors = F)
train <-read.csv("../DATA/HousingData_Train.csv", stringsAsFactors = F)
test  <-read.csv("../DATA/HousingData_Test.csv", stringsAsFactors = F)

```

# Section 2: Check and cleanse the data
``` {r echo=FALSE, message=FALSE}
## Structure of the data

dim(train)
str(train)

dim(test)
str(test)
```

``` {r echo=FALSE, message=FALSE}
#Count the number of columns that consists of text data

sum(sapply(train[,1:81], typeof) == "character")
```

``` {r echo=FALSE, message=FALSE}

sum(sapply(train[,1:81], typeof) == "integer")
```

``` {r echo=FALSE, message=FALSE}
summary(train[,sapply(train[,1:81], typeof) == "integer"])
```

Add analysis here.

In order to gain further insights into the structure of the dataset, including missing values (NA), we can summarize the dataset with the skim function in the skimr package. As seen below, there are 43 character columns, and 38 numeric columns. There are a total of 1460 rows in the dataframe. Some highlights from the skim summary:

Then we will check the data types and correct any that are the wrong type.
We also notice that the variable MSSubClass was parsed as numeric, but it is actually a categorical variables so it must be converted to a character variable. The variables OverallQual and OverallCond are also categorical, even though they seem numeric variables with a meaninful order. However, since there is no sense of the relative distance between rankings, we must convert them to character variables. Same reasoning applies to variables involving year and month in numeric form: YearBuilt, YearRemodAdd, GarageYrBlt, MoSold, YrSold. Also, the Id variable is just an index column and we will make this explicit.


# Section 3: Data Visualisation

```{r eval = TRUE, echo=TRUE, comment=NA, warning=FALSE, message=FALSE,results='markup'}

plot(SalePrice ~ LotArea, data= train) #plot of Sale Price vs. Lot Area in training data
# This first graph lets us discover that there are many outliers that could get in the way of our prediction accuracy (mainly values over 700,000 and Lots with an area of 100,000 ft or more)

plot(SalePrice ~ YearBuilt, data= train) #plot of Sale Price vs. Year that the house was built in training data

plot(SalePrice ~ YearRemodAdd, data= train) #plot of Sale Price vs. Year the house was last remodelede in training data

hist(train$SalePrice) #histogram of sales prices of houses

#histogram shows us that the price of houses is skewed towards the left, with most houses being sold for under $200,000

houses.train<-subset(train, Id<=1100)
houses.predict<-subset(train, Id>=1100)

fit<-lm(SalePrice ~ MSSubClass+LotFrontage+LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+MasVnrArea+BsmtFinSF1+BsmtFinSF2+BsmtUnfSF+TotalBsmtSF+X1stFlrSF+X2ndFlrSF+GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageYrBlt+GarageArea+WoodDeckSF+OpenPorchSF, data=houses.train)

predicted.SalePrices<-predict(fit, houses.predict)

plot(predicted.SalePrices ~ houses.predict$SalePrice) #plot predictions vs. values in testing data
abline(0,0) #draw the line at 0

summary(fit) #summary statistics of first linear regression

par(mfrow=c(1,4))
plot(fit)

#Now we will transform our data into a data.Matrix in order to be able to take out the correlation matrix and identify variables with strong correlation
outTRAIN <- data.matrix(train)

library(corrplot) #call the library that will be used to plot the correlation matrix

res <- cor(outTRAIN) #compute the correlation matrix in our training data and do case-wise deletion of missing variables
round(res, 4) #make sure that the values in our correlation matrix are rounded to 4 decimal places


##Will now assess quality of the model by performing cross-validation and calculating the MAPE ##

houses.data.test<-subset(train, (Id >= 800 & Id <= 1090))
houses.data.train<-subset(train, Id <= 800) #redefine the training data

fit <- lm(SalePrice ~ MSSubClass+LotFrontage+LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+MasVnrArea+BsmtFinSF1+BsmtFinSF2+BsmtUnfSF+TotalBsmtSF+X1stFlrSF+X2ndFlrSF+GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageYrBlt+GarageArea+WoodDeckSF+OpenPorchSF, data = houses.data.train)

predicted.prices.testing<-predict(fit, houses.data.test) #predict the prices of the 1000 diamonds left for testing the model


percent.errors <- abs((houses.data.test$SalePrice-predicted.prices.testing)/houses.data.test$Price)*100 #calculate absolute percentage errors


#fit.log <- lm(log(SalePrice ~ MSSubClass+LotFrontage+LotArea+OverallQual+OverallCond+YearBuilt+YearRemodAdd+MasVnrArea+BsmtFinSF1+BsmtFinSF2+BsmtUnfSF+TotalBsmtSF+X1stFlrSF+X2ndFlrSF+GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Fireplaces+GarageYrBlt+GarageArea+WoodDeckSF+OpenPorchSF, data = houses.data.train))

#predicted.prices.testing.log<-predict(fit, houses.data.test) 

#percent.errors.log <- abs((houses.data.test$SalePrice-predicted.prices.testing.log)/houses.data.test$Price)*100 #calculate absolute percentage errors

#mean(percent.errors.log) 


```

# Section 4: Missing Values

Character variables Alley, FireplaceQu, PoolQC, Fence, MiscFeature have >50% empty values so we will dummy variables them.The missing values indicate that majority of the houses have no alley access, no pool, no fence and no elevator, no 2nd garage, no shed and no tennis court that is covered by the MiscFeature.

Combining train and test data for quicker data prep

```{r, message=FALSE, warning=FALSE}
test$SalePrice <- NA  #Because we need both data sets to be the same length
train$isTrain <- 1  #This way we'll know what is what
test$isTrain <- 0  #This way we'll know what is what
house <- rbind(train,test)
```

## MasVnrArea !!!!! we might want to change this

```{r, message=FALSE, warning=FALSE}
house$MasVnrArea[which(is.na(house$MasVnrArea))] <- mean(house$MasVnrArea,na.rm=T)
```


## Alley

Changing NA in Alley to None

```{r, message=FALSE, warning=FALSE}
house$Alley1 <- as.character(house$Alley)
house$Alley1[which(is.na(house$Alley))] <- "None"
table(house$Alley1)
house$Alley <- as.factor(house$Alley1)
house <- subset(house,select = -Alley1)
```

## MasVnrType

Changing NA in MasVnrType to None

```{r, message=FALSE, warning=FALSE}
house$MasVnrType1 <- as.character(house$MasVnrType)
house$MasVnrType1[which(is.na(house$MasVnrType))] <- "None"
house$MasVnrType <- as.factor(house$MasVnrType1)
house <- subset(house,select = -MasVnrType1)
table(house$MasVnrType)
```

## LotFrontage

Imputing missing Lot Frontage by the median

```{r, message=FALSE, warning=FALSE}
house$LotFrontage[which(is.na(house$LotFrontage))] <- median(house$LotFrontage,na.rm = T)
```

## FireplaceQu

Changing NA in FireplaceQu to None

```{r, message=FALSE, warning=FALSE}
house$FireplaceQu1 <- as.character(house$FireplaceQu)
house$FireplaceQu1[which(is.na(house$FireplaceQu))] <- "None"
house$FireplaceQu <- as.factor(house$FireplaceQu1)
house <- subset(house,select = -FireplaceQu1)
```

## PoolQC

Changing NA in PoolQC to None

```{r, message=FALSE, warning=FALSE}
house$PoolQC1 <- as.character(house$PoolQC)
house$PoolQC1[which(is.na(house$PoolQC))] <- "None"
house$PoolQC <- as.factor(house$PoolQC1)
house <- subset(house,select = -PoolQC1)
```

## Fence

Changing NA in Fence to None

```{r, message=FALSE, warning=FALSE}
house$Fence1 <- as.character(house$Fence)
house$Fence1[which(is.na(house$Fence))] <- "None"
house$Fence <- as.factor(house$Fence1)
house <- subset(house,select = -Fence1)
```

## MiscFeature

Changing NA in MiscFeature to None

```{r, message=FALSE, warning=FALSE}
house$MiscFeature1 <- as.character(house$MiscFeature)
house$MiscFeature1[which(is.na(house$MiscFeature))] <- "None"
house$MiscFeature <- as.factor(house$MiscFeature1)
house <- subset(house,select = -MiscFeature1)
```

## GarageType

Changing NA in GarageType to None

```{r, message=FALSE, warning=FALSE}
house$GarageType1 <- as.character(house$GarageType)
house$GarageType1[which(is.na(house$GarageType))] <- "None"
house$GarageType <- as.factor(house$GarageType1)
house <- subset(house,select = -GarageType1)
```

## GarageYrBlt

Changing NA in GarageYrBlt to None

```{r, message=FALSE, warning=FALSE}
house$GarageYrBlt[which(is.na(house$GarageYrBlt))] <- 0 
```

## GarageFinish

Changing NA in GarageFinish to None

```{r, message=FALSE, warning=FALSE}
house$GarageFinish1 <- as.character(house$GarageFinish)
house$GarageFinish1[which(is.na(house$GarageFinish))] <- "None"
house$GarageFinish <- as.factor(house$GarageFinish1)
house <- subset(house,select = -GarageFinish1)
```

## GarageQual

Changing NA in GarageQual to None

```{r, message=FALSE, warning=FALSE}
house$GarageQual1 <- as.character(house$GarageQual)
house$GarageQual1[which(is.na(house$GarageQual))] <- "None"
house$GarageQual <- as.factor(house$GarageQual1)
house <- subset(house,select = -GarageQual1)
```

## GarageCond

Changing NA in GarageCond to None

```{r, message=FALSE, warning=FALSE}
house$GarageCond1 <- as.character(house$GarageCond)
house$GarageCond1[which(is.na(house$GarageCond))] <- "None"
house$GarageCond <- as.factor(house$GarageCond1)
house <- subset(house,select = -GarageCond1)
```

## BsmtQual

Changing NA in BsmtQual to None

```{r, message=FALSE, warning=FALSE}
house$BsmtQual1 <- as.character(house$BsmtQual)
house$BsmtQual1[which(is.na(house$BsmtQual))] <- "None"
house$BsmtQual <- as.factor(house$BsmtQual1)
house <- subset(house,select = -BsmtQual1)
```

## BsmtCond

Changing NA in BsmtCond to None

```{r, message=FALSE, warning=FALSE}
house$BsmtCond1 <- as.character(house$BsmtCond)
house$BsmtCond1[which(is.na(house$BsmtCond))] <- "None"
house$BsmtCond <- as.factor(house$BsmtCond1)
house <- subset(house,select = -BsmtCond1)
```

## BsmtExposure

Changing NA in BsmtExposure to None

```{r, message=FALSE, warning=FALSE}
house$BsmtExposure1 <- as.character(house$BsmtExposure)
house$BsmtExposure1[which(is.na(house$BsmtExposure))] <- "None"
house$BsmtExposure <- as.factor(house$BsmtExposure1)
house <- subset(house,select = -BsmtExposure1)
```

## BsmtFinType1

Changing NA in BsmtFinType1 to None

```{r, message=FALSE, warning=FALSE}
house$BsmtFinType11 <- as.character(house$BsmtFinType1)
house$BsmtFinType11[which(is.na(house$BsmtFinType1))] <- "None"
house$BsmtFinType1 <- as.factor(house$BsmtFinType11)
house <- subset(house,select = -BsmtFinType11)
```

## BsmtFinType2

Changing NA in BsmtFinType2 to None

```{r, message=FALSE, warning=FALSE}
house$BsmtFinType21 <- as.character(house$BsmtFinType2)
house$BsmtFinType21[which(is.na(house$BsmtFinType2))] <- "None"
house$BsmtFinType2 <- as.factor(house$BsmtFinType21)
house <- subset(house,select = -BsmtFinType21)
```

## Electrical

Changing NA in Electrical to None

```{r, message=FALSE, warning=FALSE}
house$Electrical1 <- as.character(house$Electrical)
house$Electrical1[which(is.na(house$Electrical))] <- "None"
house$Electrical <- as.factor(house$Electrical1)
house <- subset(house,select = -Electrical1)
```

## Factorizing

```{r, message=FALSE, warning=FALSE}
house$MSZoning<- factor(house$MSZoning)
house$Street <- factor(house$Street)
house$LotShape <-factor(house$LotShape )
house$LandContour<-factor(house$LandContour)
house$Utilities<-factor(house$Utilities)
house$LotConfig<-factor(house$LotConfig)
house$LandSlope<-factor(house$LandSlope)
house$Neighborhood<-factor(house$Neighborhood)
house$Condition1<-factor(house$Condition1)
house$Condition2<-factor(house$Condition2)
house$BldgType<-factor(house$BldgType)
house$HouseStyle<-factor(house$HouseStyle)
house$RoofStyle<-factor(house$RoofStyle)
house$RoofMatl<-factor(house$RoofMatl)
house$Exterior1st<-factor(house$Exterior1st)
house$Exterior2nd<-factor(house$Exterior2nd)
house$ExterQual<-factor(house$ExterQual)
house$ExterCond<-factor(house$ExterCond)
house$Foundation<-factor(house$Foundation)
house$Heating<-factor(house$Heating)
house$HeatingQC<-factor(house$HeatingQC)
house$CentralAir<-factor(house$CentralAir)
house$KitchenQual<-factor(house$KitchenQual)
house$Functional<-factor(house$Functional)
house$PavedDrive<-factor(house$PavedDrive)
house$SaleType<-factor(house$SaleType)
house$SaleCondition<-factor(house$SaleCondition)
str(house)
```


Taking all the column classes in one variable so as to separate factors from numerical variables.

```{r, message=FALSE, warning=FALSE}
Column_classes <- sapply(names(house),function(x){class(house[[x]])})
numeric_columns <-names(Column_classes[Column_classes != "factor"])
```

Train and test dataset creation

```{r, message=FALSE, warning=FALSE}
train <- house[house$isTrain==1,]
test <- house[house$isTrain==0,]
smp_size <- floor(0.75 * nrow(train))

## setting the seed to make the partition reproducible

set.seed(123)
train_ind <- sample(seq_len(nrow(train)), size = smp_size) 

train_new <- train[train_ind, ] # this is what we should use to train
validate <- train[-train_ind, ] # this is what we should use to validate
train_new <- subset(train_new,select=-c(Id,isTrain))
validate <- subset(validate,select=-c(Id,isTrain))
nrow(train_new)
nrow(validate)
ncol(train_new)
ncol(validate)
str(validate)
```

## 5.....TODO - need to decide what to do here

```{r, message=FALSE, warning=FALSE}
# Looking at the distribution and summary of the target variable

summary(train$SalePrice)

quantile(train$SalePrice)

# Conclusion: From summary, it was observed that minimum price is greater than 0

## Histogram for target variable

hist(train$SalePrice)

## Conclusion: From Histogram, we could see that it deviates from normal distribution and has positive skewness.


# Plotting 'GrLivArea' too see if there are any outliers !!!! Consider moving this earlier TODO

ggplot(train,aes(y=SalePrice,x=GrLivArea))+geom_point()
summary(train$GrLivArea)

# There are outliers in 'GrLivArea' field. Let's remove those outliers.

train <- train[train$GrLivArea<=4000,]
```


## 7.....Build the model

## ?. Linear
```{r, message=FALSE, warning=FALSE}
# XXX
```

## ?. LASSO
```{r, message=FALSE, warning=FALSE}
# Marcelo's work
set.seed(400)

#Splitting the data into training and test set

training.samples <- train$SalePrice %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- train[training.samples, ]
test.data <- train[training.samples, ]

#predictor variables
##ERROR x <- model.matrix(SalePrice~., train.data)[,-1]
#outcome variable
##ERROR y <- train.data$SalePrice

#compute penalized linear regression models
## NOT YET RUN cv <- cv.glmnet(x, y, alpha = 0)
#NOT YET RUN cv$lambda.min



```

## ?. Random Forrest
```{r, message=FALSE, warning=FALSE}
 library(randomForest)
 house_model <- randomForest(SalePrice~.,
                            data = train_new)
```

## ?. XGBoost
```{r, message=FALSE, warning=FALSE}
library(xgboost)
library(caret)
library(ROCR)
library(lift)

#model_XGboost<-xgboost(data = data.matrix(training.x[,-1]), 
#                       label = as.numeric(as.character(train_new$SalePrice)), 
#                       eta = 0.1,       # hyperparameter: learning rate 
#                       max_depth = 20,  # hyperparameter: size of a tree in each boosting iteration
#                       nround=50,       # hyperparameter: number of boosting iterations  
#                       objective = "reg:squarederror"
#                       )


training_xgb <-model.matrix(SalePrice~ ., data = train_new)
validate_xgb <-model.matrix(SalePrice~ ., data = validate)
#test_xgb <- subset(test,select=-c(Id,isTrain))
test_xgb <- model.matrix(SalePrice~ ., data = test)

training_xgb <- xgb.DMatrix(data = as.matrix(training_xgb), label= train_new$SalePrice)
validate_xgb <- xgb.DMatrix(data = as.matrix(validate_xgb))
test_xgb <- xgb.DMatrix(data = as.matrix(test_xgb))

default_param<-list(
  objective = "reg:squarederror",
  booster = "gbtree",
  eta=0.1, #default = 0.3
  gamma=0,
  max_depth=20, #default=6
  min_child_weight=1, #default=1
  subsample=1,
  colsample_bytree=1
)

xgbcv <- xgb.cv( params = default_param, data = training_xgb, 
                 nrounds = 50, nfold = 5, showsd = T, 
                 stratified = T, print_every_n = 40, 
                 early_stopping_rounds = 10, maximize = F)

xgb_mod <- xgb.train(data = training_xgb, params=default_param, nrounds = 50)

xgb_pred <- predict(xgb_mod, validate_xgb)

xgb_pred

xgb_pred <- predict(xgb_mod, test_xgb)

xgb_pred


```

## 8...Variable importance TODO (this is only for random forrest)

Get importance

```{r, message=FALSE, warning=FALSE}
importance    <- importance(house_model)
varImpPlot(house_model)
```

## 9...Final Prediction

```{r, message=FALSE, warning=FALSE}
# Predict using the test set

prediction <- predict(house_model,validate)

# Evaluation RMSE function

RMSE <- function(x,y){
  a <- sqrt(sum((log(x)-log(y))^2)/length(y))
  return(a)
}
```

RMSE

```{r, message=FALSE, warning=FALSE}
RMSE1 <- RMSE(prediction, validate$SalePrice)
RMSE1
RMSE1 <- round(RMSE1, digits = 5)
```

Output file

```{r, message=FALSE, warning=FALSE}

prediction <- predict(house_model,test)

prediction[which(is.na(prediction))] <- mean(prediction,na.rm=T)
submit <- data.frame(Id=test$Id,SalePrice=prediction)
write.csv(submit,file="House_Price_Output.csv",row.names=F)
```